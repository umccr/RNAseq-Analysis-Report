---
title: "UMCCR WTS-WGS Results Mapping"
author: "UMCCR"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: false
    toc_float: false
    code_folding: hide
params:
  project: "Avner"
  secondary: 
  sample_name: "CCR170115b_MH17T002P033_RNA"
---

```{r custom, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(knitr)
library(tidyr)
library(rmarkdown)
library(dplyr)
library(readr)
library(forcats)
library(stringr)
library(janitor)
library(googledrive)
library(here)
library(skimr)
library(purrr)
```

## Introduction

Code snippets to generate result summaries for WTS-WGS runs for RNAseq report generation, based on a [Google Spreadsheet](https://docs.google.com/spreadsheets/d/1DwvyfVrgr5TIcYtGVXZeIWWRbld-nuX-4o4z2kZHNWs/edit#gid=0), aka the dreadful LIMS stand-in. 

### 1. Setting up project information

This currently requires specifying the project name and will pull out all files of that type and project which do not have results associated with them yet. This should be generalized at some point to support sample extraction by Illumina RunID or by patient ID. The exact names can be copied from the `Project` column of the [Google-LIMS sheet](https://docs.google.com/spreadsheets/u/1/d/1aaTvXrZSdA1ekiLEpW60OeNq2V7D_oEMBzTgC-uDJAM/edit#gid=0). The `googledrive` framework requires authentication with oAuth. This can be done interactive, but storing a token simplifies the process; see the [googlesheet authentication vignette](https://rawgit.com/jennybc/googlesheets/master/vignettes/managing-auth-tokens.html) for details. 

An alternative use is to set the "secondary" analysis flag to match the samples that need to be processed, regardless of processing status. The `PROJECT` name will be used to name config and sample files, but any sample with the matching `SECONDARY` entry in the `secondary analysis` column of the spreadsheet will be added. This is useful when re-processing samples for research projects. Long term, the idea is that we get rid of this filtering step completely and just generate sync lists and templates for all samples that still need to be processed, then mark the processing stage in Google-LIMS to avoid duplication.

```{r project, eval=FALSE}
PROJECT <- params$project

if ( is.null(params$secondary) ) {
  
  SECONDARY <- ""
  
} else {
  SECONDARY <- params$secondary
}


```

### 2. Import data from Google spreadsheet

This step generates a data frame from the Google spreadsheet, unifying variable names and simplifying subject IDs for bcbio along the way. It replaces empty `results` cells (which indicate samples that still need processing) with a `-` to distinguish from true NAs and gets rid of whitespace in subject identifiers (although ideally there should not be any empty cells in the sheet in the first place). Otherwise standard data cleanup with `janitor`. We are also creating a timestamped backup of the Google Doc each time it is accessed, just in case.

```{r importData, message=FALSE, eval=FALSE}
# Register UMCCR spreadsheet. Use cached authentication
#gs_auth(token = "./googlesheets_token_umccr.rds")

# Create a backup copy each time we access the sample info
tm <- as.POSIXlt(Sys.time(), "UTC", "%Y-%m-%dT%H:%M:%S")
timestamp <- strftime(tm , "%Y-%m-%dT%H%M")
filename <- paste(timestamp, 'backup.csv', sep = '_')
backup_dir <- file.path(here::here('output', 'backup'))
dir.create(backup_dir, recursive = TRUE) # mkdir -p (warns if it already exists):

# Google Drive implementation
lims_key <- drive_find('^Google LIMS$', team_drive = 'LIMS')$id
drive_download(as_id(lims_key), path = file.path(backup_dir, filename), overwrite = TRUE)

# Import downloaded spreadsheet (sheet #1) as a tibble
samples_gs <- read_csv(file.path(backup_dir, filename))

# Tweak for analysis
samples <- samples_gs %>%
  clean_names() %>%
  mutate(secondary_analysis = ifelse(is.na(secondary_analysis), '-', secondary_analysis)) %>%
  remove_empty(c('rows', 'cols')) %>%
  mutate(subject_id = gsub(' ', '.', subject_id)) %>%
  mutate(results = ifelse(is.na(results), '-', results)) %>%
  dplyr::select(-matches('number_fastqs')) # Drop FASTQ count introduced with new version for now

# Add unique id to each row
samples <- samples %>%
  mutate(row_id = rownames(samples))
```

### 3. Find results ready for reporting

Find all samples that belong to the provided project and that have been processed:

```{r subsetSamples, eval=FALSE}
# Keep rows matching project and type requested; extract path to FASTQ
# and generate new file name for these
bcbio <- samples %>%
  dplyr::filter(((project == PROJECT & results != '-') | 
           (secondary_analysis == SECONDARY))) %>% 
  dplyr::select(fastq, results, run, project, sample_id, sample_name, subject_id, type, phenotype, row_id) %>%
  dplyr::mutate(targetname = paste(sample_name, sep = '_')) %>%
  dplyr::mutate(targetname = str_replace_all(targetname, '-', '_')) 

```

### 4. Remove topup and normal samples

Samples with the exact same name (but from different runs) are expected to be top-ups and would have been merged with original samples during upstream processing. Also, WGS has normal samples, which are not required for the mapping.

```{r removeTopups, eval=FALSE}
# Remove top-up samples (samples with the exact same description)
# before calculating WTS-WGS result assignments.
# This assumes topups are consistently flagged with a `_topup`
# suffix
template <- bcbio[!(stringr::str_detect(bcbio$sample_id, pattern = '_topup$')), ]
template <- template[!(template$type=='WGS' & template$phenotype=='normal'), ]
```

### 5. Generating templates for WTS-WGS results mapping

Generate a file with pointers to the sample FASTQs and their WGS-WTS result path (if both available).

